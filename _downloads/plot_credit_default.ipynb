{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\n==============================================================\nExample: detecting defaults on retail credits with skope_rules\n==============================================================\n\n\nSkopeRules finds logical rules with high precision and fuse them. Finding\ngood rules is done by fitting classification or regression trees\nto sub-samples.\nA fitted tree defines a set of rules (each tree node defines a rule); rules\nare then tested out of the bag, and the ones with higher precision are kept.\nThis set of rules is  decision function, reflecting for\neach new samples how many rules have find it abnormal.\n\nThis example aims at finding logical rules to predict credit defaults. The\nanalysis shows that setting.\n\nThe dataset comes from BLABLABLA.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Data import and preparation\n..................\n\nThere are 3 categorical variables (SEX, EDUCATION and MARRIAGE) and 20\nnumerical variables.\nThe target (credit defaults) is transformed in a binary variable with\nintegers 0 (no default) and 1 (default).\nFrom the 30000 credits, 50% are used for training and 50% are used\nfor testing. The target is unbalanced with a 22%/78% ratio.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.utils import shuffle\nfrom skrules import SkopeRules\nfrom skrules.datasets import load_credit_data\n\nprint(__doc__)\n\nrng = np.random.RandomState(42)\n\n# Importing data\ndataset = load_credit_data()\nX = dataset.data\ny = dataset.target\n# Shuffling data, preparing target and variables\ndata, y = shuffle(np.array(X), y)\ndata = pd.DataFrame(data, columns=X.columns)\n\nfor col in ['ID']:\n    del data[col]\n\n# data = pd.get_dummies(data, columns = ['SEX', 'EDUCATION', 'MARRIAGE'])\n\n# Quick feature engineering\ndata = data.rename(columns={\"PAY_0\": \"PAY_1\"})\nold_PAY = ['PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\ndata['PAY_old_mean'] = data[old_PAY].apply(lambda x: np.mean(x), axis=1)\n\nold_BILL_AMT = ['BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\ndata['BILL_AMT_old_mean'] = data[old_BILL_AMT].apply(\n    lambda x: np.mean(x), axis=1)\ndata['BILL_AMT_old_std'] = data[old_BILL_AMT].apply(\n    lambda x: np.std(x),\n    axis=1)\n\nold_PAY_AMT = ['PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\ndata['PAY_AMT_old_mean'] = data[old_PAY_AMT].apply(\n    lambda x: np.mean(x), axis=1)\ndata['PAY_AMT_old_std'] = data[old_PAY_AMT].apply(\n    lambda x: np.std(x), axis=1)\n\ndata = data.drop(old_PAY_AMT, axis=1)\ndata = data.drop(old_BILL_AMT, axis=1)\ndata = data.drop(old_PAY, axis=1)\n\n# Creating the train/test split\nfeature_names = list(data.columns)\nprint(feature_names)\ndata = data.values\nn_samples = data.shape[0]\nn_samples_train = int(n_samples / 2)\ny_train = y[:n_samples_train]\ny_test = y[n_samples_train:]\nX_train = data[:n_samples_train]\nX_test = data[n_samples_train:]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Benchmark with a Decision Tree and Random Forests\n..................\n\nThis part shows the training and performance evaluation of\ntwo tree-based models.\nThe objective remains to extract rules which targets credit defaults.\nThis benchmark shows the performance reached with a decision tree and a\nrandom forest.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "DT = GridSearchCV(DecisionTreeClassifier(),\n                  param_grid={\n                  'max_depth': range(3, 10, 1),\n                  'min_samples_split': range(10, 1000, 200),\n                  'criterion': [\"gini\", \"entropy\"]},\n                  scoring={'AUC': 'roc_auc'}, cv=5, refit='AUC',\n                  n_jobs=-1)\n\nDT.fit(X_train, y_train)\nscoring_DT = DT.predict_proba(X_test)[:, 1]\n\nRF = GridSearchCV(\n    RandomForestClassifier(\n        n_estimators=30,\n        class_weight='balanced'),\n    param_grid={\n        'max_depth': range(2, 7, 1),\n        'max_features': np.linspace(0.1, 0.5, 5)\n        },\n    scoring={'AUC': 'roc_auc'}, cv=5,\n    refit='AUC', n_jobs=-1)\n\nRF.fit(X_train, y_train)\nscoring_RF = RF.predict_proba(X_test)[:, 1]\n\nprint(\"Decision Tree selected parameters : \"+str(DT.best_params_))\nprint(\"Random Forest selected parameters : \"+str(RF.best_params_))\n\n# Plot ROC and PR curves\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5),\n                         sharex=True, sharey=True)\n\ncurves = [roc_curve, precision_recall_curve]\nxlabels = ['False Positive Rate', 'Recall (True Positive Rate)']\nylabels = ['True Positive Rate (Recall)', 'Precision']\n\nfor ax, curve, xlabel, ylabel in zip(axes.flatten(),\n                                     curves, xlabels, ylabels):\n    if curve == precision_recall_curve:\n        y_dt, x_dt, _ = curve(y_test, scoring_DT)\n        y_rf, x_rf, _ = curve(y_test, scoring_RF)\n        ax.scatter(x_dt, y_dt, c='b', s=10)\n        ax.step(x_rf, y_rf, linestyle='-.', c='g', lw=1, where='post')\n        ax.set_title(\"Precision-Recall Curves\", fontsize=20)\n    else:\n        x_dt, y_dt, _ = curve(y_test, scoring_DT)\n        x_rf, y_rf, _ = curve(y_test, scoring_RF)\n        label = ('Decision Tree, AUC: %0.3f' % auc(x_dt, y_dt))\n        ax.scatter(x_dt, y_dt, c='b', s=10, label=label)\n        label = ('Random Forest, AUC: %0.3f' % auc(x_rf, y_rf))\n        ax.plot(x_rf, y_rf, '-.', lw=1, label=label, c='g')\n        ax.set_title(\"ROC Curves\", fontsize=20)\n        ax.legend(loc='upper center', fontsize=8)\n\n    ax.set_xlabel(xlabel, fontsize=18)\n    ax.set_ylabel(ylabel, fontsize=18)\n\nplt.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The ROC and Precision-Recall curves show that both models\napproximatively have the same performances for low True Positive Rates\n(also called Recall).\nThis \"low-recall\" domain is visible around the bottom right hand side\nof the ROC curve and on the left side of the Precision-Recall curve.\nA good performance on this part of the curve means that the model can\nprecisely detect a fraction of credit defaults (the easiest one).\nFor highest recalls, Random Forests shows a better performance\nin this domain.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Getting rules with skrules\n..................\n\nThis part shows how SkopeRules can be fitted to detect credit defaults.\nPerformances are compared with the random forest model previously trained.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# fit the model\nrng = np.random.RandomState(42)\n\nclf = SkopeRules(\n    similarity_thres=1.0, max_depth=3, max_features=0.5,\n    max_samples_features=0.5, random_state=rng, n_estimators=30,\n    feature_names=feature_names, recall_min=0.02, precision_min=0.6\n    )\nclf.fit(X_train, y_train)\nscoring = clf.decision_function(X_test)\n\nprint(str(len(clf.rules_)) + ' rules have been built.')\nprint('The most precise rules are the following:')\nprint(clf.rules_[:5])\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5),\n                         sharex=True, sharey=True)\n\ncurves = [roc_curve, precision_recall_curve]\nxlabels = ['False Positive Rate', 'Recall (True Positive Rate)']\nylabels = ['True Positive Rate (Recall)', 'Precision']\n\nfor ax, curve, xlabel, ylabel in zip(axes.flatten(), curves,\n                                     xlabels, ylabels):\n    if curve == precision_recall_curve:\n        y_sr, x_sr, _ = curve(y_test, scoring)\n        y_rf, x_rf, _ = curve(y_test, scoring_RF)\n        ax.scatter(x_sr, y_sr, c='b', s=10, label=label)\n        ax.step(x_rf, y_rf, linestyle='-.', c='g', lw=1, where='post')\n        ax.set_title(\"Precision-Recall Curves\", fontsize=20)\n    else:\n        x_sr, y_sr, _ = curve(y_test, scoring)\n        x_rf, y_rf, _ = curve(y_test, scoring_RF)\n        label = ('SkopeRules, AUC: %0.3f' % auc(x_sr, y_sr))\n        ax.scatter(x_sr, y_sr, c='b', s=10, label=label)\n        label = ('Random Forest, AUC: %0.3f' % auc(x_rf, y_rf))\n        ax.plot(x_rf, y_rf, '-.', lw=1, label=label, c='g')\n        ax.set_title(\"ROC Curves\", fontsize=20)\n        ax.legend(loc='upper center', fontsize=8)\n\n    ax.set_xlabel(xlabel, fontsize=18)\n    ax.set_ylabel(ylabel, fontsize=18)\n\nplt.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Refining rules with the similarity threshold parameter\n..................\n\nThis part shows how SkopeRules can be set up to discard unecessary rules.\nThis rule selection consists in\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# fit the model\nrng = np.random.RandomState(42)\n\nclf = SkopeRules(\n    similarity_thres=1.0, max_depth=3, max_features=0.5,\n    max_samples_features=0.5, random_state=rng, n_estimators=30,\n    feature_names=feature_names, recall_min=0.05, precision_min=0.6\n    )\n\nclf.fit(X_train, y_train)\nscoring = clf.decision_function(X_test)\n\nrng = np.random.RandomState(42)\n\nclf = SkopeRules(\n    similarity_thres=0.9, max_depth=3, max_features=0.5,\n    max_samples_features=0.5, random_state=rng, n_estimators=30,\n    feature_names=feature_names, recall_min=0.05, precision_min=0.6\n    )\nclf.fit(X_train, y_train)\nscoring_RF = clf.decision_function(X_test)\n\nrng = np.random.RandomState(42)\n\nclf = SkopeRules(\n    similarity_thres=0.5, max_depth=3, max_features=0.5,\n    max_samples_features=0.5, random_state=rng, n_estimators=30,\n    feature_names=feature_names, recall_min=0.05, precision_min=0.6\n    )\nclf.fit(X_train, y_train)\nscoring_ET = clf.decision_function(X_test)\n\n# Plot models\nfig, axes = plt.subplots(1, 2, figsize=(12, 5),\n                         sharex=True, sharey=True)\n\ncurves = [roc_curve, precision_recall_curve]\nxlabels = ['False Positive Rate', 'Recall (True Positive Rate)']\nylabels = ['True Positive Rate (Recall)', 'Precision']\n\nfor ax, curve, xlabel, ylabel in zip(axes.flatten(), curves,\n                                     xlabels, ylabels):\n    if curve == precision_recall_curve:\n        y_sr1, x_sr1, _ = curve(y_test, scoring)\n        y_sr2, x_sr2, _ = curve(y_test, scoring_RF)\n        y_sr3, x_sr3, _ = curve(y_test, scoring_ET)\n        ax.scatter(x_sr1, y_sr1, c='b', s=10, label=label)\n        ax.step(x_sr2, y_sr2, linestyle='-.', c='g', lw=1, where='post')\n        ax.scatter(x_sr3, y_sr3, c='r', s=10, label=label)\n        ax.set_title(\"Precision-Recall Curves\", fontsize=20)\n    else:\n        x_sr1, y_sr1, _ = curve(y_test, scoring)\n        x_sr2, y_sr2, _ = curve(y_test, scoring_RF)\n        x_sr3, y_sr3, _ = curve(y_test, scoring_ET)\n        label = ('SkopeRules, AUC: %0.3f' % auc(x_sr1, y_sr1))\n        ax.scatter(x_sr1, y_sr1, c='b', s=10, label=label)\n        label = ('Random Forest, AUC: %0.3f' % auc(x_sr2, y_sr2))\n        ax.plot(x_sr2, y_sr2, '-.', lw=1, label=label, c='g')\n        label = ('ExtraTrees, AUC: %0.3f' % auc(x_sr3, y_sr3))\n        ax.scatter(x_sr3, y_sr3, c='r', s=10, label=label)\n        ax.set_title(\"ROC Curves\", fontsize=20)\n        ax.legend(loc='upper center', fontsize=8)\n\n    ax.set_xlabel(xlabel, fontsize=18)\n    ax.set_ylabel(ylabel, fontsize=18)\n\nplt.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Applying rules and predicting with skrules\n..................\n\nThis part shows how, once fitted, SkopeRules can be used to\nmake predictions.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.11", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}